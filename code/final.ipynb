{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de51861-4221-4ba6-b4c5-baa7f4e2cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_HOME='/kaggle'\n",
    "\n",
    "# UPDATE THE CODE BELOW BEFORE RUNNING LOCALLY\n",
    "# PROJECT_HOME='/Users/athul/git/map-kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b075533-d255-436b-9724-252da6c02d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "#Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c807b88-0cbf-495c-b8f1-c6cf4c7b8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# baseline - SVM\n",
    "def train_svm(X_train, y_train, random_state=RANDOM_STATE, verbosity=True):\n",
    "    clf = SVC(random_state=random_state, verbose=verbosity)\n",
    "    clf.fit(X_train_flattened, y_train)\n",
    "    return clf\n",
    "\n",
    "# baseline - logistic regression\n",
    "def train_logistic_regression(X_train, y_train, random_state=RANDOM_STATE, max_iter=100, verbosity=1):\n",
    "    hyperparameters = []\n",
    "    clf = LogisticRegression(max_iter=max_iter, verbose=verbosity, random_state=random_state)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2627f5f-b8fb-4ac7-aeea-83e7c28cfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def add_gaussian_noise(X_imgs):\n",
    "    gaussian_noise_imgs = []    \n",
    "    for X_img in X_imgs:\n",
    "        noise = tf.random.normal(shape=tf.shape(X_img), mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "        noise_img = X_img + noise\n",
    "        noise_img = tf.clip_by_value(noise_img, 0.0, 1.0)\n",
    "        gaussian_noise_imgs.append(noise_img)\n",
    "    gaussian_noise_imgs = np.array(gaussian_noise_imgs, dtype = np.float32)\n",
    "    return gaussian_noise_imgs\n",
    "\n",
    "def rotate(X_imgs, rotation_range=90):\n",
    "    rotated_imgs = []\n",
    "    for X_img in X_imgs:\n",
    "        rotated_img = tf.keras.image.random_rotation(\n",
    "            X_img, rotation_range, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest',\n",
    "            cval=0.0, interpolation_order=1)\n",
    "        rotated_imgs.append(rotated_img)\n",
    "    return rotated_imgs\n",
    "\n",
    "def zoom(X_imgs, zoom_range=(0.5,1.3)):\n",
    "    zoomed_imgs = []\n",
    "    for X_img in X_imgs:\n",
    "        zoomed_img = tf.keras.image.random_zoom(\n",
    "            X_img, zoom_range, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "        zoomed_imgs.append(zoomed_img)\n",
    "    return zoomed_imgs\n",
    "\n",
    "def horizontal_flip(X_imgs):\n",
    "    flipped_imgs = []\n",
    "    for X_img in X_imgs:\n",
    "        flipped_img = tf.image.flip_left_right(X_img)\n",
    "        flipped_imgs.append(X_img)\n",
    "    return flipped_imgs\n",
    "  \n",
    "def vertical_flip(X_imgs):\n",
    "    flipped_imgs = []\n",
    "    for X_img in X_imgs:\n",
    "        flipped_img = tf.image.flip_up_down(X_img)\n",
    "        flipped_imgs.append(X_img)\n",
    "    return flipped_imgs\n",
    "\n",
    "\n",
    "def selective_augment(X_imgs, y_labels, times=2):\n",
    "    augmented_data_X = np.concatenate((X_imgs, horizontal_flip(X_imgs)))\n",
    "    augmented_data_Y = np.concatenate((y_labels, y_labels))\n",
    "    if times==3: # add more samples to triple the dataset \n",
    "        vertical_flipped_imgs = vertical_flip(X_imgs)\n",
    "        augmented_data_X = np.concatenate((augmented_data_X, vertical_flipped_imgs))\n",
    "        augmented_data_Y = np.concatenate((augmented_data_Y, y_labels))\n",
    "    return (augmented_data_X, augmented_data_Y)\n",
    "\n",
    "def get_image_data_generator(rotation_range=30, zoom_range=[0.8, 1.2], brightness_range=[0.8,1.0]):\n",
    "    data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "\t\trotation_range=rotation_range, zoom_range=zoom_range,\n",
    "\t\tbrightness_range=brightness_range)\n",
    "    print('Returning image data generator')\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1c0091-73a8-4501-b0c3-fd2e46b1d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "\ty_pred_flattened = np.argmax(y_pred, axis=1)\n",
    "\ty_true_flattened = np.argmax(y_true, axis=1)\n",
    "\treturn f1_score(y_true_flattened, y_pred_flattened, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81439aec-2ad9-4313-b020-2f19f5976fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN models\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def m1():\n",
    "    model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(96,96,1),padding='same'),\n",
    "    # BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    MaxPooling2D((2, 2),padding='same'),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='linear',padding='same',kernel_regularizer=l2(0.01)),\n",
    "    # BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='linear',padding='same'),\n",
    "    # BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),          \n",
    "    MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='linear',padding='same',kernel_regularizer=l2(0.01)),\n",
    "    # BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),                 \n",
    "    MaxPooling2D(pool_size=(2, 2),padding='same'),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='linear'),\n",
    "    # BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),  \n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')])\n",
    "    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def m2(num_classes=11):\n",
    "    model_master = Sequential()\n",
    "\n",
    "    model_master.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(96,96,1), padding=\"same\"))\n",
    "    model_master.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "    model_master.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    model_master.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    model_master.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    model_master.add(Flatten())\n",
    "    model_master.add(Dense(84, activation='relu'))\n",
    "    model_master.add(Dense(num_classes, activation='softmax'))\n",
    "    return model_master\n",
    "\n",
    "def m3():\n",
    "    model_master = Sequential()\n",
    "    model_master.add(BatchNormalization())\n",
    "\n",
    "    model_master.add(Conv2D(96, kernel_size=(3, 3), activation='relu', input_shape=(96,96,1), kernel_initializer='he_normal', padding=\"same\"))\n",
    "    model_master.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "    model_master.add(Conv2D(96, (3, 3), activation='relu', padding = 'same'))\n",
    "    model_master.add(BatchNormalization())\n",
    "    model_master.add(MaxPooling2D(pool_size=(3, 3), strides = 2))\n",
    "\t\n",
    "    model_master.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model_master.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model_master.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model_master.add(BatchNormalization())\n",
    "    model_master.add(MaxPooling2D(pool_size=(3, 3), strides = 2))\n",
    "\n",
    "    model_master.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model_master.add(Conv2D(192, (1, 1), activation='relu'))\n",
    "    model_master.add(Conv2D(11, (1, 1)))\n",
    "    model_master.add(BatchNormalization())\n",
    "    model_master.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model_master.add(Activation(activation='softmax'))\n",
    "    return model_master\n",
    "\n",
    "def m4():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same',input_shape=(96,96,1)))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2))\n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2))\n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(192, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(11, (1, 1)))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation(activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def m5() :\n",
    "    model = Sequential()\n",
    "\n",
    "    #mlpconv block 1\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu',padding='valid', input_shape = (96,96,1)))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #mlpconv block2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',padding='valid'))\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #mlpconv block3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu',padding='valid'))\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu'))\n",
    "    model.add(Conv2D(11, (1, 1)))\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation(activation='softmax'))    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# CNN\n",
    "def create_cnn(num_classes):\n",
    "    model = Sequential([\n",
    "        \n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(96,96,1), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling2D((2, 2), padding='same'),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        LeakyReLU(alpha=0.1),          \n",
    "        MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        LeakyReLU(alpha=0.1),                 \n",
    "        MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        LeakyReLU(alpha=0.1),  \n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def simple_cnn(input_shape=(96, 96, 1), num_classes=11):\n",
    "\t\n",
    "\t# building a linear stack of layers with the sequential model\n",
    "\tmodel = Sequential()\n",
    "\t# convolutional layer\n",
    "\tmodel.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=input_shape))\n",
    "\tmodel.add(MaxPool2D(pool_size=(1,1)))\n",
    "\t# flatten output of conv\n",
    "\tmodel.add(Flatten())\n",
    "\t# hidden layer\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\t# output layer\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\treturn model\n",
    "\n",
    "def simple_cnn2(input_shape=(96, 96, 1), num_classes=11):\n",
    "\t\n",
    "\t# building a linear stack of layers with the sequential model\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\t# convolutional layer\n",
    "\tmodel.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=input_shape))\n",
    "\n",
    "\t# convolutional layer\n",
    "\tmodel.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "\tmodel.add(MaxPool2D(pool_size=(2,2)))\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\tmodel.add(Conv2D(250, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "\tmodel.add(MaxPool2D(pool_size=(2,2)))\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t# flatten output of conv\n",
    "\tmodel.add(Flatten())\n",
    "\n",
    "\t# hidden layer\n",
    "\tmodel.add(Dense(500, activation='relu'))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Dense(250, activation='relu'))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t# output layer\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "def resnet50(input_shape, num_classes=11):\n",
    "    return tf.keras.applications.ResNet50V2(weights=None,\n",
    "        input_shape=input_shape, pooling=None, classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a185f27f-8cce-4ec1-a49b-5c8d7d0fd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def encode(y):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "    return le.transform(y)\n",
    "\n",
    "def encode_to_categorical(y, num_classes=11):\n",
    "\ty_encoded = encode(y)\n",
    "\ty_encoded_categorical = to_categorical(y_encoded, num_classes)\n",
    "\treturn y_encoded_categorical\n",
    "\n",
    "def flatten(dataframe):\n",
    "    return dataframe.reshape(dataframe.shape[0], dataframe.shape[1] * dataframe.shape[2])\n",
    "\n",
    "def balance(X, y):\n",
    "    augmented_data_X = np.empty((0, X.shape[1], X.shape[2], X.shape[3]))\n",
    "    augmented_data_Y = []\n",
    "    freq = np.unique(y, return_counts=True)\n",
    "    for index in range(len(freq[0])):\n",
    "        label_index = np.where(y == freq[0][index])\n",
    "        if freq[1][index] < 700:\n",
    "            augmented_data = selective_augment(X[label_index], y[label_index], 3)\n",
    "        elif freq[1][index] < 1000:\n",
    "            augmented_data = selective_augment(X[label_index], y[label_index])\n",
    "        else:\n",
    "            augmented_data = (X[label_index], y[label_index])\n",
    "\n",
    "        augmented_data_X = np.concatenate((augmented_data_X, augmented_data[0]))\n",
    "        augmented_data_Y = np.concatenate((augmented_data_Y, augmented_data[1]))\n",
    "    return augmented_data_X, augmented_data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9661df-a95c-44cd-bf9c-f830a46f0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "SUBMISSION_HEADER = ['Id', 'class']\n",
    "\n",
    "def open_pickled_file(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pd.read_pickle(file)\n",
    "    return np.asarray(data)\n",
    "\n",
    "def export_predictions(y_pred):\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = PROJECT_HOME + '/output/prediction-{}.csv'.format(timestr)\n",
    "    file = open(filename, 'w+', newline ='')\n",
    "    with file:    \n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(SUBMISSION_HEADER)\n",
    "        writer.writerows(enumerate(y_pred))\n",
    "    print('Predictions exported to {}'.format(filename))\n",
    "\t\n",
    "def show_image(data):\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def check_dataset(X, y, count=1):\n",
    "    randomlist = random.sample(range(len(X)), count)\n",
    "    for i in randomlist:\n",
    "        show_image(X[i])\n",
    "        print(y[i])\n",
    "\t\t\n",
    "def predict_and_export(clf, X):\n",
    "    print('Predicting..')\n",
    "    y_pred = clf.predict(X)\n",
    "    print('Exporting predictions..')\n",
    "    export_predictions(y_pred)\n",
    "    print('Export completed')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88bc987c-9a59-4f89-bfc9-27172da51927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy(model):\n",
    "\t# accuracy of our model\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tplt.plot(model.history[\"accuracy\"])\n",
    "\tplt.plot(model.history[\"val_accuracy\"])\n",
    "\tplt.xlabel(\"Epochs\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.title(\"ACCURACY OF MODEL\")\n",
    "\tplt.legend(['training_accuracy', 'validation_accuracy'])\n",
    "\tplt.show()\n",
    "\t\n",
    "def plot_loss(model):\n",
    "\t# loss of our model\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tplt.plot(model.history[\"loss\"])\n",
    "\tplt.plot(model.history[\"val_loss\"])\n",
    "\tplt.xlabel(\"Epochs\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.title(\"LOSS OF MODEL\")\n",
    "\tplt.legend(['training_loss', 'validation_loss'])\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a5c7b5-43a4-423a-b8c0-2e8346060e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X_dataset, y_dataset, test_size=0.2, random_state=0, shuffle=False):\n",
    "    return train_test_split(X_dataset, y_dataset, test_size=test_size, random_state=random_state, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979682b-08ce-4f05-8483-17cf19f04a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8281e91d-345d-4a2a-80e1-d920fec984da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = open_pickled_file(PROJECT_HOME + '/input/fall2021-inf8245e-machine-learning/x_train.pkl')\n",
    "y_train_raw = open_pickled_file(PROJECT_HOME + '/input/fall2021-inf8245e-machine-learning/y_train.pkl')\n",
    "X_test_raw = open_pickled_file(PROJECT_HOME + '/input/fall2021-inf8245e-machine-learning/x_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02bb02a6-68e8-4ce4-8c61-749a9a0e5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_raw.shape)\n",
    "# print(y_train_raw.shape)\n",
    "# print(X_test_raw.shape)\n",
    "# freq = np.unique(y_train_raw, return_counts=True)\n",
    "# print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6859729c-1aaa-4882-8230-a7896387a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing data...\n"
     ]
    }
   ],
   "source": [
    "#scale and flatten X\n",
    "\n",
    "X_train_scaled = X_train_raw/255\n",
    "X_test_scaled = X_test_raw/255\n",
    "\n",
    "X_train_scaled_reshaped = X_train_scaled.reshape(-1, 96, 96, 1)\n",
    "X_test_scaled_reshaped = X_test_scaled.reshape(-1, 96, 96, 1)\n",
    "\n",
    "# Required to train baseline models\n",
    "# X_train_scaled_flattened = flatten(X_train_scaled)\n",
    "# X_test_scaled_flattened = flatten(X_test_scaled)\n",
    "\n",
    "#encode y_train\n",
    "y_train_encoded = encode(y_train_raw)\n",
    "Y_train_encoded_categorical = to_categorical(y_train_encoded, 11)\n",
    "\n",
    "print(\"Balancing data...\")\n",
    "balanced_dataset = balance(X_train_scaled_reshaped, y_train_encoded)\n",
    "\n",
    "X_train, X_val, y_train, y_val = split_dataset(balanced_dataset[0], to_categorical(balanced_dataset[1], 11), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "943a21a5-41ad-4c40-8092-f220c193836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below section to train baseline models. Make sure the inputs are flattened\n",
    "\n",
    "# print('Training Logistic regression')\n",
    "# clf = train_loy_predc_regression(X_train, y_train)\n",
    "# predict_and_export(clf, X_test_flattened)\n",
    "\n",
    "# print('Training SVM')\n",
    "# clf = train_svm(X_train, y_train)\n",
    "# predict_and_export(clf, X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8036f6be-41da-4369-be42-152e349d41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning image data generator\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE=16\n",
    "image_data_generator = get_image_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b91d931-cde3-4033-b2cc-940ea3f6d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN\n",
      "Returning image data generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 17:12:46.376355: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-12 17:12:46.376528: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2021-12-12 17:12:48.789208: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-12 17:12:48.800523: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 17:12:49.325416: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847/847 [==============================] - ETA: 0s - loss: 2.0818 - accuracy: 0.3167 - f1_m: 0.0056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 17:17:08.679445: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847/847 [==============================] - 275s 323ms/step - loss: 2.0818 - accuracy: 0.3167 - f1_m: 0.0056 - val_loss: 2.0822 - val_accuracy: 0.2853 - val_f1_m: 0.0142\n",
      "Epoch 2/100\n",
      "730/847 [========================>.....] - ETA: 36s - loss: 1.9204 - accuracy: 0.4177 - f1_m: 0.0226"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rg/_mt1j0m11f5glr0c1pl90vth0000gn/T/ipykernel_39504/1477252704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     Model = model.fit(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training CNN')\n",
    "tf.test.is_built_with_cuda()\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "epochs = 100\n",
    "\n",
    "filepath = './weights/' + '.{epoch:02d}-{loss:.2f}.hdf5'\n",
    "mcp_save = ModelCheckpoint(filepath, monitor='val_f1_m', mode='max')\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_f1_m', \n",
    "    min_delta=0.001,\n",
    "    patience=3)\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode=\"max\",\n",
    "    min_delta=0.01,\n",
    "    cooldown=0,\n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "model = m3()\n",
    "image_data_generator = get_image_data_generator(rotation_range=30, zoom_range=[0.9, 1.1], brightness_range=[0.8, 1])\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy', f1_m])\n",
    "    Model = model.fit(\n",
    "        image_data_generator.flow(X_train, y_train, shuffle=True, batch_size=TRAIN_BATCH_SIZE),\n",
    "        validation_data=image_data_generator.flow(X_val, y_val, shuffle=True, batch_size=TRAIN_BATCH_SIZE), \n",
    "        epochs=epochs, \n",
    "        shuffle=True, \n",
    "        callbacks=[reduce_lr_callback, mcp_save], \n",
    "        batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "    plot_accuracy(Model)\n",
    "    plot_loss(Model)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    f1_s = f1_micro(y_val, y_pred)\n",
    "    print(\"Validation f1-score:\", f1_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ebb270-6a60-4698-9b7a-124515b862f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 17:21:19.443133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions exported to /Users/athul/git/map-kaggle/output/prediction-20211212-172240.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict and export the results on test data\n",
    "pred = model.predict(X_test_scaled_reshaped)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "export_predictions(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00efa59-bcfc-40b8-bbb2-a0af376af143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
